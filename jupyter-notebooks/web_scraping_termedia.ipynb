{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web_scraping_termedia.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reliable News Web Scraping"
      ],
      "metadata": {
        "id": "sWX52MqKG-F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "id": "HzmxJcT_Nmi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "8tvEW04wFcyz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffbb3d60-4f4d-4632-9005-aa217ba3b942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import List, Tuple"
      ],
      "metadata": {
        "id": "eK4OYUTUFmPR"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_articles_links(page: int) -> List[str]:\n",
        "    \"\"\"Gets links to articles from termedia.pl.\n",
        "\n",
        "    Args:\n",
        "        page (int): Page number of termedia.pl to process.\n",
        "\n",
        "    Returns:\n",
        "        links (List[str]): List of links to articles.\n",
        "    \"\"\"\n",
        "    url = 'https://www.termedia.pl/koronawirus/?&p=' + str(page)\n",
        "    print(f'Processing page {page}: {url}')\n",
        "    res = requests.get(url)  \n",
        "    res.encoding = 'utf-8'                           \n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "    links = ['https://www.termedia.pl' + art.find('a')['href'].strip() \n",
        "                for art in soup.find_all('div', attrs={'class': 'pl2Pos'})]\n",
        "    return links\n",
        "\n",
        "\n",
        "def extract_text_from_article(url: str) -> Tuple[str, str]:\n",
        "    \"\"\"Gets the article's title and content.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL of the article.\n",
        "\n",
        "    Returns:\n",
        "        (Tuple [str, str]): Tuple containing:\n",
        "            title (str): Article's title.\n",
        "            text (str): Article's content.\n",
        "    \"\"\"\n",
        "    res = requests.get(url)\n",
        "    res.encoding = 'utf-8'                             \n",
        "    soup_art = BeautifulSoup(res.text, 'html.parser')\n",
        "    title = soup_art.find('div', attrs={'class': 'pageTitle'}).text\n",
        "    try:\n",
        "        text = ' '.join([t.text for t in soup_art.find_all('div', attrs={'class': 'articleContent'})])\n",
        "    except AttributeError:\n",
        "        text = ''\n",
        "    return title, text"
      ],
      "metadata": {
        "id": "ktNuhAU39LXA"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_counter = 0\n",
        "pages_to_scrap = 144\n",
        "\n",
        "df = pd.DataFrame(columns=['Verdict', 'Title', 'Text', 'Url'])\n",
        "\n",
        "for page in range(1, pages_to_scrap + 1):\n",
        "    links = get_articles_links(page)\n",
        "    for idx, link in enumerate(links):\n",
        "        articles_counter += 1\n",
        "        title, text = extract_text_from_article(link)\n",
        "        df.loc[10 * (page-1) + idx] = ['true', title, text, link]\n",
        "    \n",
        "df.to_excel('termedia_dataset.xlsx', encoding='utf-8', index=False)\n",
        "display(df.head())\n",
        "print(f'Scraped articles in total: {len(df)}')"
      ],
      "metadata": {
        "id": "E4YUEtrcFsZl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}