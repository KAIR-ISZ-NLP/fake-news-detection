{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_preprocessing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPLnrfjtH5eoTKHJZcc7UdH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Data preprocessing and lemmatisation"],"metadata":{"id":"S5l74exVP1hQ"}},{"cell_type":"code","source":["! pip install polyglot\n","! pip install pyicu\n","! pip install Morfessor       \n","! pip install pycld2   \n","! polyglot download LANG:pl"],"metadata":{"id":"ucLD45TBjvv-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640099250936,"user_tz":-60,"elapsed":119408,"user":{"displayName":"Konrad Golemo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFK5MavABUVG_y3Lf4h30zAe1p4-VOjOFy_Mx4=s64","userId":"02863586711249810016"}},"outputId":"65348e97-bfd7-4998-e82e-98cdcdd566d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting polyglot\n","  Downloading polyglot-16.7.4.tar.gz (126 kB)\n","\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 16.7 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 81 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 112 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 126 kB 11.8 MB/s \n","\u001b[?25hBuilding wheels for collected packages: polyglot\n","  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for polyglot: filename=polyglot-16.7.4-py2.py3-none-any.whl size=52578 sha256=c5a3e563196ba0d36a2f809adbd65c0275b386141f042f916416fcf68c2dfc40\n","  Stored in directory: /root/.cache/pip/wheels/09/bc/67/75c9de8e9726460bc0b101ad225ad025cb8ce9e0759beb9d52\n","Successfully built polyglot\n","Installing collected packages: polyglot\n","Successfully installed polyglot-16.7.4\n","Collecting pyicu\n","  Downloading PyICU-2.8.tar.gz (299 kB)\n","\u001b[K     |████████████████████████████████| 299 kB 9.9 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: pyicu\n","  Building wheel for pyicu (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyicu: filename=PyICU-2.8-cp37-cp37m-linux_x86_64.whl size=1376696 sha256=506d812fc8600232bac85da01fb8d0c55fb1c6a91e53979a4c7bc739db8f7234\n","  Stored in directory: /root/.cache/pip/wheels/14/bd/45/aeddc643bd0637c14fa27bffaee5b411cdc323f8bec76ad15e\n","Successfully built pyicu\n","Installing collected packages: pyicu\n","Successfully installed pyicu-2.8\n","Collecting Morfessor\n","  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n","Installing collected packages: Morfessor\n","Successfully installed Morfessor-2.0.6\n","Collecting pycld2\n","  Downloading pycld2-0.41.tar.gz (41.4 MB)\n","\u001b[K     |████████████████████████████████| 41.4 MB 1.7 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pycld2\n","  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycld2: filename=pycld2-0.41-cp37-cp37m-linux_x86_64.whl size=9834267 sha256=2e71b53c6bc761fae20b69a630e80ede0e6025d9cb23693331ea1c86ec4f35bd\n","  Stored in directory: /root/.cache/pip/wheels/ed/e4/58/ed2e9f43c07d617cc81fe7aff0fc6e42b16c9cf6afe960b614\n","Successfully built pycld2\n","Installing collected packages: pycld2\n","Successfully installed pycld2-0.41\n","[polyglot_data] Downloading collection 'LANG:pl'\n","[polyglot_data]    | \n","[polyglot_data]    | Downloading package sgns2.pl to\n","[polyglot_data]    |     /root/polyglot_data...\n","[polyglot_data]    | Downloading package unipos.pl to\n","[polyglot_data]    |     /root/polyglot_data...\n","[polyglot_data]    | Downloading package ner2.pl to\n","[polyglot_data]    |     /root/polyglot_data...\n","[polyglot_data]    | Downloading package counts2.pl to\n","[polyglot_data]    |     /root/polyglot_data...\n","[polyglot_data]    | Downloading package transliteration2.pl to\n","[polyglot_data]    |     /root/polyglot_data...\n","[polyglot_data]    | Downloading package embeddings2.pl to\n","[polyglot_data]    |     /root/polyglot_data...\n","[polyglot_data]    | Downloading package uniemb.pl to\n","[polyglot_data]    |     /root/polyglot_data...\n","[polyglot_data]    | Downloading package sentiment2.pl to\n","[polyglot_data]    |     /root/polyglot_data...\n","[polyglot_data]    | Downloading package tsne2.pl to\n","[polyglot_data]    |     /root/polyglot_data...\n","[polyglot_data]    | Downloading package morph2.pl to\n","[polyglot_data]    |     /root/polyglot_data...\n","[polyglot_data]    | \n","[polyglot_data]  Done downloading collection LANG:pl\n"]}]},{"cell_type":"code","source":["# Imports\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from polyglot.text import Text\n","from polyglot.detect import Detector\n","import string\n","# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization\n","drive.mount('/content/drive') "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nXaqUM5ekEck","executionInfo":{"status":"ok","timestamp":1640099320833,"user_tz":-60,"elapsed":69907,"user":{"displayName":"Konrad Golemo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFK5MavABUVG_y3Lf4h30zAe1p4-VOjOFy_Mx4=s64","userId":"02863586711249810016"}},"outputId":"47858986-5e8e-4c29-e95d-6658fa3fbd9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aET8QYDqhTvt"},"outputs":[],"source":["def drop_title_and_url(df):\n","    return df.drop(columns=['Title', 'Url'])\n","\n","def drop_empty(df):\n","    df.dropna(inplace=True)\n","    df.reset_index(drop=True, inplace=True)\n","    return df\n","\n","def drop_non_polish(df):\n","    for index, row in df.iterrows():\n","        text = row['Text']\n","        detector = Detector(text, quiet=True)\n","        if not (detector.language.name == 'Polish' and \n","                detector.language.confidence >= 70):\n","            df.drop([index], inplace=True)\n","    df.reset_index(drop=True, inplace=True)\n","    return df\n","\n","def drop_unidentified(df):\n","    return df[df['Verdict'] != 'unidentified']\n","\n","def drop_twitter(df):\n","    return df[df['Text'].str.contains('Nowy na Twitterze')==False]"]},{"cell_type":"code","source":["# Load fake news dataset\n","df_fake = pd.read_excel('drive/MyDrive/Fake News Detection/data/fakehunter_dataset.xlsx')\n","\n","# Drop irrelevant columns \n","df_fake = drop_title_and_url(df_fake)\n","\n","# Drop rows containing NaNs\n","df_fake = drop_empty(df_fake)\n","\n","# Drop news with \"unidentified\" verdict\n","df_fake = drop_unidentified(df_fake)\n","\n","# Drop non-polish news\n","df_fake = drop_non_polish(df_fake)\n","\n","# Drop noisy twitter news\n","df_fake = drop_twitter(df_fake)"],"metadata":{"id":"eVz80XO2C4ng","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640099322567,"user_tz":-60,"elapsed":1747,"user":{"displayName":"Konrad Golemo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFK5MavABUVG_y3Lf4h30zAe1p4-VOjOFy_Mx4=s64","userId":"02863586711249810016"}},"outputId":"c4211fc6-e058-4d44-b3f5-bb9f0c93f6f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n","Detector is not able to detect the language reliably.\n"]}]},{"cell_type":"code","source":["# Load reliable news dataset\n","df_real = pd.read_excel('drive/MyDrive/Fake News Detection/data/termedia_dataset.xlsx')\n","\n","# Drop irrelevant columns\n","df_real = drop_title_and_url(df_real)\n","\n","# Drop rows containing NaNs\n","df_real = drop_empty(df_real)"],"metadata":{"id":"_2vmunlMDE-V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create complete dataset (containing both fake and real news) and reset index\n","df = pd.concat([df_fake, df_real])\n","df.reset_index(drop=True, inplace=True)\n","\n","# Transform \"Verdict\" column to boolean \n","df['Verdict'].replace('false', 0, inplace=True)\n","df['Verdict'].replace('true', 1, inplace=True)\n","df['Verdict'] = df['Verdict'].astype(bool)"],"metadata":{"id":"m1sfD_TvmhRD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install morfeusz2"],"metadata":{"id":"i8FQFFcWpUpN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640099328396,"user_tz":-60,"elapsed":4254,"user":{"displayName":"Konrad Golemo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFK5MavABUVG_y3Lf4h30zAe1p4-VOjOFy_Mx4=s64","userId":"02863586711249810016"}},"outputId":"cd451cd8-52e7-4fc9-ee57-ddf7abfe3199"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting morfeusz2\n","  Downloading morfeusz2-1.99.3-20211121-cp35.cp36.cp37.cp38.cp39-abi3-manylinux2014_x86_64.whl (8.5 MB)\n","\u001b[K     |████████████████████████████████| 8.5 MB 9.0 MB/s \n","\u001b[?25hInstalling collected packages: morfeusz2\n","Successfully installed morfeusz2-1.99.3\n"]}]},{"cell_type":"code","source":["# Imports\n","import re\n","import string\n","import morfeusz2"],"metadata":{"id":"OwSQeRyZpbpB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def strip_punctuation(text):\n","#     return text.translate(str.maketrans('', '', string.punctuation))\n","\n","def strip_non_alphanumeric(text):\n","    reg = re.compile('[^a-zA-ZĄąĆćĘęŁłŃńÓóŚśŹźŻż]')\n","    return reg.sub(' ', text)\n","\n","def replace_whitespace(text):\n","    reg = re.compile('\\s+')\n","    return reg.sub(' ', text)\n","\n","def delete_escape_chars(text):\n","    return text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\r', ' ')\n","\n","def delete_stop_words(text):\n","    stop_words_txt = open('drive/MyDrive/Fake News Detection/data/polish.stopwords.txt')\n","    stop_words = stop_words_txt.read().split('\\n')\n","    stop_words_txt.close()\n","    return [word for word in text if word not in stop_words]\n","\n","def preprocess(df):\n","    # df['Text'] = df['Text'].apply(strip_punctuation)\n","    df['Text'] = df['Text'].apply(strip_non_alphanumeric)\n","    df['Text'] = df['Text'].apply(replace_whitespace)\n","    df['Text'] = df['Text'].apply(delete_escape_chars)\n","    df['Text'] = df['Text'].apply(str.lower)\n","    df['Text'] = df['Text'].apply(str.split)\n","    df['Text'] = df['Text'].apply(delete_stop_words)\n","    return df\n","\n","def lemmatise(df):\n","    morf = morfeusz2.Morfeusz()\n","    for index, row in df.iterrows():\n","        text = row['Text']\n","        lemm_words = []\n","        for word in text:\n","            _, _, interpretation = morf.analyse(word)[0]\n","            lem_word = interpretation[1]\n","            lem_word_stripped = lem_word.split(':', 1)[0].lower()\n","            lemm_words.append(lem_word_stripped)\n","        df.loc[index, 'Text'] = ' '.join(lemm_words)\n","    return df"],"metadata":{"id":"Y375IeUQIxY9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = preprocess(df)\n","df = lemmatise(df)"],"metadata":{"id":"zXP_oECpL5iu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop rows not containing text and reset index\n","df = df[df['Text'].astype(bool)]\n","df.reset_index(drop=True, inplace=True)\n","\n","# Drop rows with text shorter than 30 chars\n","df = df[df['Text'].apply(len) >= 30]\n","\n","# Display number of fake and real news\n","print(f'Number of fake news: \\t' + str(len(df[df['Verdict'] == False])))\n","print(f'Number of real news: \\t' + str(len(df[df['Verdict'] == True])))\n","print(f'Total number of news: \\t' + str(len(df)))\n","\n","# Export complete dataset\n","df.to_excel('complete_dataset.xlsx', encoding='utf-8', index=False)"],"metadata":{"id":"bzk9TE24qFXt","executionInfo":{"status":"ok","timestamp":1640099744462,"user_tz":-60,"elapsed":829,"user":{"displayName":"Konrad Golemo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFK5MavABUVG_y3Lf4h30zAe1p4-VOjOFy_Mx4=s64","userId":"02863586711249810016"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a772c6ec-1dc1-4e63-e4bd-fb2ba1a55dc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of fake news: \t753\n","Number of real news: \t1487\n","Total number of news: \t2240\n"]}]}]}