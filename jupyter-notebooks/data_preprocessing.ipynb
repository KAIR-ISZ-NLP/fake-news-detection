{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing and lemmatisation"
      ],
      "metadata": {
        "id": "S5l74exVP1hQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install polyglot\n",
        "! pip install pyicu\n",
        "! pip install Morfessor       \n",
        "! pip install pycld2   \n",
        "! polyglot download LANG:pl"
      ],
      "metadata": {
        "id": "ucLD45TBjvv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from polyglot.text import Text\n",
        "from polyglot.detect import Detector\n",
        "import string\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization\n",
        "drive.mount('/content/drive') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXaqUM5ekEck",
        "outputId": "47858986-5e8e-4c29-e95d-6658fa3fbd9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aET8QYDqhTvt"
      },
      "outputs": [],
      "source": [
        "def drop_title_and_url(df):\n",
        "    return df.drop(columns=['Title', 'Url'])\n",
        "\n",
        "def drop_empty(df):\n",
        "    df.dropna(inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n",
        "\n",
        "def drop_non_polish(df):\n",
        "    for index, row in df.iterrows():\n",
        "        text = row['Text']\n",
        "        detector = Detector(text, quiet=True)\n",
        "        if not (detector.language.name == 'Polish' and \n",
        "                detector.language.confidence >= 70):\n",
        "            df.drop([index], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n",
        "\n",
        "def drop_unidentified(df):\n",
        "    return df[df['Verdict'] != 'unidentified']\n",
        "\n",
        "def drop_twitter(df):\n",
        "    return df[df['Text'].str.contains('Nowy na Twitterze')==False]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fake news dataset\n",
        "df_fake = pd.read_excel('drive/MyDrive/Fake News Detection/data/fakehunter_dataset.xlsx')\n",
        "\n",
        "# Drop irrelevant columns \n",
        "df_fake = drop_title_and_url(df_fake)\n",
        "\n",
        "# Drop rows containing NaNs\n",
        "df_fake = drop_empty(df_fake)\n",
        "\n",
        "# Drop news with \"unidentified\" verdict\n",
        "df_fake = drop_unidentified(df_fake)\n",
        "\n",
        "# Drop non-polish news\n",
        "df_fake = drop_non_polish(df_fake)\n",
        "\n",
        "# Drop noisy twitter news\n",
        "df_fake = drop_twitter(df_fake)"
      ],
      "metadata": {
        "id": "eVz80XO2C4ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load reliable news dataset\n",
        "df_real = pd.read_excel('drive/MyDrive/Fake News Detection/data/termedia_dataset.xlsx')\n",
        "\n",
        "# Drop irrelevant columns\n",
        "df_real = drop_title_and_url(df_real)\n",
        "\n",
        "# Drop rows containing NaNs\n",
        "df_real = drop_empty(df_real)"
      ],
      "metadata": {
        "id": "_2vmunlMDE-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create complete dataset (containing both fake and real news) and reset index\n",
        "df = pd.concat([df_fake, df_real])\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Transform \"Verdict\" column to boolean \n",
        "df['Verdict'].replace('false', 0, inplace=True)\n",
        "df['Verdict'].replace('true', 1, inplace=True)\n",
        "df['Verdict'] = df['Verdict'].astype(bool)"
      ],
      "metadata": {
        "id": "m1sfD_TvmhRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install morfeusz2"
      ],
      "metadata": {
        "id": "i8FQFFcWpUpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import re\n",
        "import string\n",
        "import morfeusz2"
      ],
      "metadata": {
        "id": "OwSQeRyZpbpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def strip_punctuation(text):\n",
        "#     return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def strip_non_alphanumeric(text):\n",
        "    reg = re.compile('[^a-zA-ZĄąĆćĘęŁłŃńÓóŚśŹźŻż]')\n",
        "    return reg.sub(' ', text)\n",
        "\n",
        "def replace_whitespace(text):\n",
        "    reg = re.compile('\\s+')\n",
        "    return reg.sub(' ', text)\n",
        "\n",
        "def delete_escape_chars(text):\n",
        "    return text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\r', ' ')\n",
        "\n",
        "def delete_stop_words(text):\n",
        "    stop_words_txt = open('drive/MyDrive/Fake News Detection/data/polish.stopwords.txt')\n",
        "    stop_words = stop_words_txt.read().split('\\n')\n",
        "    stop_words_txt.close()\n",
        "    return [word for word in text if word not in stop_words]\n",
        "\n",
        "def preprocess(df):\n",
        "    # df['Text'] = df['Text'].apply(strip_punctuation)\n",
        "    df['Text'] = df['Text'].apply(strip_non_alphanumeric)\n",
        "    df['Text'] = df['Text'].apply(replace_whitespace)\n",
        "    df['Text'] = df['Text'].apply(delete_escape_chars)\n",
        "    df['Text'] = df['Text'].apply(str.lower)\n",
        "    df['Text'] = df['Text'].apply(str.split)\n",
        "    df['Text'] = df['Text'].apply(delete_stop_words)\n",
        "    return df\n",
        "\n",
        "def lemmatise(df):\n",
        "    morf = morfeusz2.Morfeusz()\n",
        "    for index, row in df.iterrows():\n",
        "        text = row['Text']\n",
        "        lemm_words = []\n",
        "        for word in text:\n",
        "            _, _, interpretation = morf.analyse(word)[0]\n",
        "            lem_word = interpretation[1]\n",
        "            lem_word_stripped = lem_word.split(':', 1)[0].lower()\n",
        "            lemm_words.append(lem_word_stripped)\n",
        "        df.loc[index, 'Text'] = ' '.join(lemm_words)\n",
        "    return df"
      ],
      "metadata": {
        "id": "Y375IeUQIxY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = preprocess(df)\n",
        "df = lemmatise(df)"
      ],
      "metadata": {
        "id": "zXP_oECpL5iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows not containing text and reset index\n",
        "df = df[df['Text'].astype(bool)]\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Drop rows with text shorter than 30 chars\n",
        "df = df[df['Text'].apply(len) >= 30]\n",
        "\n",
        "# Display number of fake and real news\n",
        "print(f'Number of fake news: \\t' + str(len(df[df['Verdict'] == False])))\n",
        "print(f'Number of real news: \\t' + str(len(df[df['Verdict'] == True])))\n",
        "print(f'Total number of news: \\t' + str(len(df)))\n",
        "\n",
        "# Export complete dataset\n",
        "df.to_excel('complete_dataset.xlsx', encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "bzk9TE24qFXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a772c6ec-1dc1-4e63-e4bd-fb2ba1a55dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of fake news: \t753\n",
            "Number of real news: \t1487\n",
            "Total number of news: \t2240\n"
          ]
        }
      ]
    }
  ]
}