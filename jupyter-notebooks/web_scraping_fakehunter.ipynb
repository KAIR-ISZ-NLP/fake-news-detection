{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web_scraping_fakehunter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58LuNqn6BjAs"
      },
      "source": [
        "#Fake News Web Scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEWnt9oYI9hD"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICXIxzTMZuUV"
      },
      "source": [
        "! pip install requests\n",
        "! pip install easyocr\n",
        "! pip install opencv-python-headless==4.1.2.30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLilzCuJ6TQK"
      },
      "source": [
        "# Imports\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import easyocr\n",
        "from typing import Dict"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_verifications(page: int) -> Dict[str, str]:\n",
        "    \"\"\"Gets news verifications from fakehunter.pap.pl.\n",
        "\n",
        "    Args:\n",
        "        page (int): Page number of fakehunter.pap.pl to process.\n",
        "\n",
        "    Returns:\n",
        "        verifications (Dict[str, str]): Dictionary of web page content.\n",
        "    \"\"\"\n",
        "    url = 'https://panel-api.fakehunter.pap.pl/news/published/news?category=koronawirus&domains%5B%5D=koronawirus&page=' + str(page)\n",
        "    print(f'Processing page {page}: {url}')\n",
        "    res = requests.get(url)  \n",
        "    res.encoding = 'utf-8'                            \n",
        "    data = res.json()\n",
        "    verifications = data['results']\n",
        "    return verifications\n",
        "\n",
        "\n",
        "reader = easyocr.Reader(['pl'])  # Load the OCR Reader object\n",
        "\n",
        "\n",
        "def extract_text_from_image(url: str) -> str:\n",
        "    \"\"\"Performs OCR on the screenshot of an article.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL of the screenshot.\n",
        "\n",
        "    Returns:\n",
        "        text (str): Extracted text.\n",
        "    \"\"\"\n",
        "    res = requests.get(url)\n",
        "    arr = np.asarray(bytearray(res.content), dtype=np.uint8)\n",
        "    img = cv2.imdecode(arr, -1)\n",
        "    result = reader.readtext(img, decoder='beamsearch', detail=0, \n",
        "                            paragraph=True, y_ths=4, min_size=200, width_ths=1, \n",
        "                            allowlist='#0123456789ABCDEFGHIJKLŁMNOPRSŚTUVWXYZŻŹaąbcćdeęfghijklłmnńoóprstuwyzżź .,-?:-!\"()')\n",
        "    text = ' '.join(result)\n",
        "    return text"
      ],
      "metadata": {
        "id": "RUDpBeha4omU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXVBb114Bkjy"
      },
      "source": [
        "articles_counter = 0\n",
        "pages_to_scrap = 144\n",
        "\n",
        "df = pd.DataFrame(columns=['Verdict', 'Title', 'Text', 'Url'])\n",
        "\n",
        "for page in range(1, pages_to_scrap + 1):\n",
        "    verifications = get_verifications(page)\n",
        "\n",
        "    for idx, ver in enumerate(verifications):\n",
        "        articles_counter += 1\n",
        "        text = extract_text_from_image(ver['screenshot_url'])\n",
        "\n",
        "        df.loc[20 * (page-1) + idx] = [ver['expert_opinion']['verdict'], \n",
        "                                       ver['title'], text, ver['url']]\n",
        "    \n",
        "df.to_excel('fakehunter_dataset.xlsx', encoding='utf-8', index=False)\n",
        "display(df.head())\n",
        "print(f'Scraped articles in total: {len(df)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OCR example"
      ],
      "metadata": {
        "id": "gv6W77_QL2uQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "res = requests.get('https://sfnf-collector-prod.s3.amazonaws.com/c6dee03b-1547-4ef4-af42-3fc27aa69c0b.jpg')\n",
        "arr = np.asarray(bytearray(res.content), dtype=np.uint8)\n",
        "img = cv2.imdecode(arr, -1)\n",
        "results = reader.readtext(img, decoder='beamsearch', y_ths=4, paragraph=True, min_size=200, width_ths=1)\n",
        "\n",
        "# text = ' '.join(result)\n",
        "for (bbox, text) in results:\n",
        "    print(\"{:.4f}: {}\".format(prob, text))\n",
        "    (tl, tr, br, bl) = bbox\n",
        "    tl = (int(tl[0]), int(tl[1]))\n",
        "    tr = (int(tr[0]), int(tr[1]))\n",
        "    br = (int(br[0]), int(br[1]))\n",
        "    bl = (int(bl[0]), int(bl[1]))\n",
        "    cv2.rectangle(img, tl, br, (0, 0, 255), 2) \n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "BE6KY0qmL2Uc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}